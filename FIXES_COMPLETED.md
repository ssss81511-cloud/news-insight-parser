# ‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã

–í—Å–µ 4 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º—ã –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –ø–µ—Ä–µ–¥ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –Ω–∞ –Ω–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.

---

## 1. ‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –ü—Ä–æ–±–ª–µ–º–∞ (–±—ã–ª–æ):
```
‚ùå HNParser - –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
‚ùå RedditParser - –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
‚ùå PHParser - –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
‚ùå –ù–µ—Ç –æ–±—â–µ–π –ª–æ–≥–∏–∫–∏
```

### –†–µ—à–µ–Ω–∏–µ (—Å—Ç–∞–ª–æ):
```python
‚úÖ BaseParser - –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
‚úÖ –ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: fetch_posts(), normalize_post()
‚úÖ –û–±—â–∞—è –ª–æ–≥–∏–∫–∞: content_hash, importance_score
‚úÖ –õ–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
```

### –§–∞–π–ª—ã:
- `parsers/base_parser.py` - –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å

### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
```python
class RedditParser(BaseParser):
    def __init__(self):
        super().__init__('reddit')

    def fetch_posts(self, section, limit):
        # Reddit-specific –∫–æ–¥
        pass

    def normalize_post(self, raw_post):
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å Reddit ‚Üí Universal —Ñ–æ—Ä–º–∞—Ç
        return {
            'source': 'reddit',
            'source_id': raw_post['id'],
            'title': raw_post['title'],
            'content': raw_post['selftext'],
            'score': raw_post['ups'],
            # ...
        }
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –¢–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ = –ø—Ä–æ—Å—Ç–æ –Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å BaseParser!

---

## 2. ‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏

### –ü—Ä–æ–±–ª–µ–º–∞ (–±—ã–ª–æ):
```
‚ùå –û–¥–∏–Ω –ø—Ä–æ–¥—É–∫—Ç –Ω–∞ HN ‚Üí 1 –ø–æ—Å—Ç
‚ùå –¢–æ—Ç –∂–µ –ø—Ä–æ–¥—É–∫—Ç –Ω–∞ Reddit ‚Üí 1 –ø–æ—Å—Ç
‚ùå –¢–æ—Ç –∂–µ –ø—Ä–æ–¥—É–∫—Ç –Ω–∞ PH ‚Üí 1 –ø–æ—Å—Ç
= 3 –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –ø–æ—Å—Ç–∞ –≤ –ë–î
```

### –†–µ—à–µ–Ω–∏–µ (—Å—Ç–∞–ª–æ):
```python
‚úÖ UniversalPost - –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
‚úÖ content_hash - —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –æ—Ç–ø–µ—á–∞—Ç–æ–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
‚úÖ DuplicateGroup - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–æ—Å—Ç–æ–≤
```

### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:

#### –®–∞–≥ 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è content_hash
```python
def generate_content_hash(title, content):
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    text = f"{title.lower()} {content.lower()}"
    # SHA-256 —Ö–µ—à
    return hashlib.sha256(text.encode()).hexdigest()
```

#### –®–∞–≥ 2: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
```python
def _check_and_link_duplicates(new_post):
    # –ù–∞–π—Ç–∏ –ø–æ—Å—Ç—ã —Å –ø–æ—Ö–æ–∂–∏–º content_hash –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
    similar_posts = query(UniversalPost).filter(
        source != new_post.source,  # –ò–∑ –¥—Ä—É–≥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        content_hash == new_post.content_hash  # –ò–ª–∏ –ø–æ—Ö–æ–∂–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    )

    for similar_post in similar_posts:
        similarity = calculate_similarity(new_post, similar_post)

        if similarity > 0.7:  # 70% –ø–æ—Ö–æ–∂–∏
            # –°–≤—è–∑–∞—Ç—å –≤ –≥—Ä—É–ø–ø—É
            link_to_duplicate_group(new_post, similar_post)
```

#### –®–∞–≥ 3: –†–∞—Å—á—ë—Ç similarity
```
- Title similarity (50% –≤–µ—Å)
- Content similarity (30% –≤–µ—Å)
- Time proximity (20% –≤–µ—Å) - –ø–æ—Å—Ç–∏–ª–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ –æ–¥–Ω–æ –≤—Ä–µ–º—è
```

### –ü—Ä–∏–º–µ—Ä:

**Post 1 (HN):**
```
Title: "Show HN: My new SaaS for email marketing"
Content: "Built with Next.js..."
Posted: 2025-12-17 10:00
```

**Post 2 (Reddit r/SaaS):**
```
Title: "Launched my new SaaS for email marketing"
Content: "Built using Next.js..."
Posted: 2025-12-17 11:30
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```
‚úÖ Similarity: 85%
‚úÖ –°–æ–∑–¥–∞–Ω–∞ DuplicateGroup
‚úÖ –û–±–∞ –ø–æ—Å—Ç–∞ —Å–≤—è–∑–∞–Ω—ã
‚úÖ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –≤–∏–¥–µ—Ç—å: "–û–¥–∏–Ω –ø—Ä–æ–¥—É–∫—Ç –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –Ω–∞ 2 –ø–ª–æ—â–∞–¥–∫–∞—Ö"
```

### –§–∞–π–ª—ã:
- `storage/universal_models.py` - UniversalPost, DuplicateGroup
- `storage/universal_database.py` - –ª–æ–≥–∏–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏

---

## 3. ‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤

### –ü—Ä–æ–±–ª–µ–º–∞ (–±—ã–ª–æ):
```
‚ùå –í—Å–µ —Å–∏–≥–Ω–∞–ª—ã —Ä–∞–≤–Ω–æ–∑–Ω–∞—á–Ω—ã
‚ùå "pricing" —É–ø–æ–º—è–Ω—É—Ç 2 —Ä–∞–∑–∞ = "pricing" —É–ø–æ–º—è–Ω—É—Ç 20 —Ä–∞–∑
‚ùå –ù–µ—Ç –ø–æ–Ω—è—Ç–∏—è "–≤–∞–∂–Ω—ã–π" vs "–Ω–µ–≤–∞–∂–Ω—ã–π"
‚ùå 100 —Å–ª–∞–±—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞–≥–ª—É—à–∞—é—Ç 5 –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω—ã—Ö
```

### –†–µ—à–µ–Ω–∏–µ (—Å—Ç–∞–ª–æ):
```python
‚úÖ importance_score (0-100) - –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–µ–Ω —Å–∏–≥–Ω–∞–ª
‚úÖ priority (critical/high/medium/low)
‚úÖ growth_rate - —Ä–∞—Å—Ç—ë—Ç –∏–ª–∏ –ø–∞–¥–∞–µ—Ç
‚úÖ is_trending - "–≥–æ—Ä—è—á–∏–π" —Ç—Ä–µ–Ω–¥
```

### –ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è importance_score:

```python
def _calculate_signal_importance(signal):
    score = 0

    # 1. –ß–∞—Å—Ç–æ—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π (40 –±–∞–ª–ª–æ–≤)
    score += min(signal.frequency * 2, 40)

    # 2. –†–æ—Å—Ç (30 –±–∞–ª–ª–æ–≤)
    if signal.growth_rate > 0:  # –†–∞—Å—Ç—ë—Ç
        score += min(signal.growth_rate * 10, 30)

    # 3. –ö—Ä–æ—Å—Å-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤—ã–π –±–æ–Ω—É—Å (20 –±–∞–ª–ª–æ–≤)
    if signal.is_cross_source:  # HN + Reddit + PH
        score += len(sources) * 10

    # 4. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (10 –±–∞–ª–ª–æ–≤)
    score += signal.confidence_score * 0.1

    return min(score, 100)
```

### Priority levels:

```
importance >= 80 && frequency >= 10 ‚Üí CRITICAL (üî¥)
importance >= 60 && frequency >= 5  ‚Üí HIGH (üü†)
importance >= 40                    ‚Üí MEDIUM (üü°)
else                                 ‚Üí LOW (‚ö™)
```

### Trending detection:

```python
is_trending = (
    growth_rate > 0.5 &&      # –ë—ã—Å—Ç—Ä–æ —Ä–∞—Å—Ç—ë—Ç
    velocity > 1.0 &&         # –í—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–æ—Å—Ç–∞
    last_seen < 48h           # –£–ø–æ–º–∏–Ω–∞–ª—Å—è –Ω–µ–¥–∞–≤–Ω–æ
)
```

### –ü—Ä–∏–º–µ—Ä:

**Signal 1: "pricing problem"**
```
Frequency: 25
Growth rate: 0.8 (—Ä–∞—Å—Ç—ë—Ç)
Sources: ['hacker_news', 'reddit', 'product_hunt']
‚Üí importance_score: 88
‚Üí priority: CRITICAL
‚Üí is_trending: true
```

**Signal 2: "button color"**
```
Frequency: 3
Growth rate: -0.2 (–ø–∞–¥–∞–µ—Ç)
Sources: ['hacker_news']
‚Üí importance_score: 12
‚Üí priority: LOW
‚Üí is_trending: false
```

### –§–∞–π–ª—ã:
- `storage/universal_models.py` - EnhancedSignal –º–æ–¥–µ–ª—å
- `storage/universal_database.py` - —Ä–∞—Å—á—ë—Ç importance
- `analyzers/enhanced_signal_detector.py` - –¥–µ—Ç–µ–∫—Ü–∏—è

---

## 4. ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### –ü—Ä–æ–±–ª–µ–º–∞ (–±—ã–ª–æ):
```
‚ùå Signal: "pricing problem" (frequency: 10)
‚ùå –ù–û: –ß–¢–û –ò–ú–ï–ù–ù–û –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –≤ pricing?
‚ùå –ü–æ—Ç–µ—Ä—è–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç
‚ùå –ù–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
```

### –†–µ—à–µ–Ω–∏–µ (—Å—Ç–∞–ª–æ):
```python
‚úÖ context_snippets - —Å–ø–∏—Å–æ–∫ —Ü–∏—Ç–∞—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
‚úÖ example_urls - —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ—Å—Ç—ã
‚úÖ description - —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
‚úÖ keywords - –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
```

### –ö–∞–∫ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç:

```python
def _extract_context(text, keyword, window=100):
    """
    –ò–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞

    "I'm struggling with pricing my SaaS.
     It's hard to find the right model..."

    keyword = "pricing"
    window = 100 —Å–∏–º–≤–æ–ª–æ–≤

    –†–µ–∑—É–ª—å—Ç–∞—Ç:
    "...struggling with pricing my SaaS.
     It's hard to find the right model..."
    """
    pos = text.find(keyword)
    start = max(0, pos - window)
    end = min(len(text), pos + len(keyword) + window)

    return text[start:end]
```

### –ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:

**Signal: "pricing problem"**

```json
{
  "title": "Repeating pain: pricing problem",
  "frequency": 15,
  "priority": "critical",
  "importance_score": 85,

  "description": "Mentioned 15 times across 2 source(s). Common themes: strategy, model, tier",

  "context_snippets": [
    "...I'm struggling with pricing my SaaS. Hard to find the right model...",
    "...pricing is the biggest challenge. Should I do per-user or usage-based?...",
    "...customers complain pricing is too complicated. Need to simplify...",
    "...switched from monthly to annual pricing. Revenue up 40%...",
    "...pricing page redesign doubled conversions. Keep it simple..."
  ],

  "example_urls": [
    "https://news.ycombinator.com/item?id=12345",
    "https://reddit.com/r/SaaS/comments/abc123",
    "https://news.ycombinator.com/item?id=67890"
  ],

  "keywords": "pricing problem strategy model tier",

  "sources": ["hacker_news", "reddit"]
}
```

**–¢–µ–ø–µ—Ä—å –≤–∏–¥–Ω–æ:**
1. ‚úÖ –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è (15)
2. ‚úÖ –ù–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–Ω–æ (critical, 85/100)
3. ‚úÖ –†–µ–∞–ª—å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
4. ‚úÖ –°—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—Å—É–∂–¥–µ–Ω–∏—è
5. ‚úÖ –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã: strategy, model, tier
6. ‚úÖ –ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞ HN –∏ Reddit

### –§–∞–π–ª—ã:
- `analyzers/enhanced_signal_detector.py` - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

---

## üìä –ò—Ç–æ–≥–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –î–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:
```
HNParser ‚Üí HNPost ‚Üí Signal (–ø—Ä–æ—Å—Ç–æ–π)
                       ‚Üì
                   –ù–µ—Ç —Å–≤—è–∑–µ–π
                   –ù–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
                   –ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
```

### –ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:
```
BaseParser (unified interface)
  ‚Üì
  ‚îú‚îÄ HNParser     ‚Üí UniversalPost ‚îÄ‚îÄ‚îê
  ‚îú‚îÄ RedditParser ‚Üí UniversalPost ‚îÄ‚îÄ‚îº‚îÄ‚Üí DuplicateGroup (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)
  ‚îî‚îÄ PHParser     ‚Üí UniversalPost ‚îÄ‚îÄ‚îò
                       ‚Üì
                   EnhancedSignal
                   ‚îú‚îÄ importance_score
                   ‚îú‚îÄ priority
                   ‚îú‚îÄ context_snippets
                   ‚îú‚îÄ growth_rate
                   ‚îú‚îÄ is_trending
                   ‚îî‚îÄ cross-source correlation
```

---

## üéØ –ß—Ç–æ —ç—Ç–æ –¥–∞—ë—Ç

### 1. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å
–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ = 1 –∫–ª–∞—Å—Å, 2 –º–µ—Ç–æ–¥–∞:
```python
class NewSourceParser(BaseParser):
    def fetch_posts(self): ...
    def normalize_post(self): ...
# Done!
```

### 2. –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤
```
–î–æ: 100 —Å–∏–≥–Ω–∞–ª–æ–≤, –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ —á—Ç–æ –≤–∞–∂–Ω–æ
–ü–æ—Å–ª–µ: 10 critical, 20 high, 30 medium, 40 low
‚Üí –§–æ–∫—É—Å –Ω–∞ —Ç–æ–º —á—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω–æ
```

### 3. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
```
–î–æ: "pricing" —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è 10 —Ä–∞–∑
–ü–æ—Å–ª–µ: "pricing" —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è 10 —Ä–∞–∑
        + 5 —Ü–∏—Ç–∞—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏
        + —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—Å—É–∂–¥–µ–Ω–∏—è
        + –æ–±—â–∏–µ —Ç–µ–º—ã: "strategy, model, tier"
```

### 4. –ö—Ä–æ—Å—Å-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
```
–î–æ: "pricing" –Ω–∞ HN = —Å–∏–≥–Ω–∞–ª 1
    "pricing" –Ω–∞ Reddit = —Å–∏–≥–Ω–∞–ª 2
    (2 –Ω–µ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–∞)

–ü–æ—Å–ª–µ: "pricing" = 1 —Å–∏–≥–Ω–∞–ª
       ‚îú‚îÄ HN: 7 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
       ‚îú‚îÄ Reddit: 5 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
       ‚îî‚îÄ Total: 12 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
       (–µ–¥–∏–Ω—ã–π —Å–∏–≥–Ω–∞–ª —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π —á–∞—Å—Ç–æ—Ç–æ–π)
```

---

## üìÅ –ù–æ–≤—ã–µ —Ñ–∞–π–ª—ã

1. **parsers/base_parser.py**
   - –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
   - –û–±—â–∏–µ –º–µ—Ç–æ–¥—ã: content_hash, importance_score

2. **storage/universal_models.py**
   - UniversalPost - –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
   - DuplicateGroup - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
   - EnhancedSignal - —Å–∏–≥–Ω–∞–ª—ã —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏

3. **storage/universal_database.py**
   - UniversalDatabaseManager - —Ä–∞–±–æ—Ç–∞ —Å –Ω–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
   - –õ–æ–≥–∏–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
   - –†–∞—Å—á—ë—Ç importance scores

4. **analyzers/enhanced_signal_detector.py**
   - EnhancedSignalDetector - —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä
   - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
   - –ö—Ä–æ—Å—Å-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è

---

## ‚öôÔ∏è –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

### –°–µ–π—á–∞—Å –Ω—É–∂–Ω–æ:
1. **–†–µ—Ñ–∞–∫—Ç–æ—Ä–∏—Ç—å HNParser** –ø–æ–¥ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
2. **–°–æ–∑–¥–∞—Ç—å ParserOrchestrator** –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–∞—Ä—Å–µ—Ä–∞–º–∏
3. **–û–±–Ω–æ–≤–∏—Ç—å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å UniversalPost

### –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ:
‚úÖ –ì–æ—Ç–æ–≤–æ –∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é!
‚úÖ –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å Reddit, Product Hunt, Indie Hackers
‚úÖ –í—Å–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∞—Ç:
   - –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
   - –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—é
   - –ö–æ–Ω—Ç–µ–∫—Å—Ç
   - –ö—Ä–æ—Å—Å-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑

---

## üí° –í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å

**–≠—Ç–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è = —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç**

–ë–µ–∑ –Ω–∏—Ö –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–æ–∑–¥–∞—Å—Ç —Ö–∞–æ—Å:
- 5 –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö —Å–∏—Å—Ç–µ–º
- –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ–≤—Å—é–¥—É
- –®—É–º –≤–º–µ—Å—Ç–æ —Å–∏–≥–Ω–∞–ª–∞
- –ü–æ—Ç–µ—Ä—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–° –Ω–∏–º–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ = –ª–µ–≥–∫–æ:
- –û–¥–∏–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
- –£–º–Ω–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

**–ü–æ—Ç—Ä–∞—Ç–∏–ª–∏ –≤—Ä–µ–º—è —Å–µ–π—á–∞—Å ‚Üí —ç–∫–æ–Ω–æ–º–∏–º –º–µ—Å—è—Ü—ã –ø–æ—Ç–æ–º!**
